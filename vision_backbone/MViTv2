paper : [MViTv2: Improved Multiscale Vision Transformers for Classification and Detection](https://arxiv.org/pdf/2112.01526.pdf)

Pooling attention은 단순히 attention 모듈을 위한 projection이후 pooling 을 통해서 sequence length를 축소시킨다.
또한 attention feature에 pooled q 를 residual connection을 통해서 더해준다.
Swin에서는 window간의 feature 연산을 위해서 shifted window local attention을 해주지만 해당 논문에서는 
각 스테이지의 마지막 블럭을 pooling attention으로 대체해줌으로써 FPN의 input 으로 들어가는 feature maps은 global information을 
가질 수 있도록 한다.

이 논문을 보면서 느낀 점은 일단 pooling ~ unpooling이 대세라고 생각이 들고 해당 모듈 결과 손실되는 정보를 어떻게 잘 커버할 수 있을지가 
중요한 문제라고 생각하고 이러한 문제들을 극복하려는 쪽으로 연구해보자!
