paper : [MViTv2: Improved Multiscale Vision Transformers for Classification and Detection](https://arxiv.org/pdf/2112.01526.pdf)

Pooling attention은 단순히 attention 모듈을 위한 projection이후 pooling 을 통해서 sequence length를 축소시킨다.
